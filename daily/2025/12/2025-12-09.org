#+title: 2025-12-09
#+filetags: :daily:

* 直近の予定締め切り

* TASK
** INBOX LT大会パワポ
** INBOX レポート作成→つくば
** INBOX ポスター作成
** INBOX レポート12月分の作成
*** 1._結果と反省
*** 2._今後の展望
** DONE 津田さんに結果報告メッセージ→今書いてスケジュールで送る
** misc
*** DONE 定期申し込み
*** DONE パスポート顔写真
*** WAIT 奨学金
→朝やる必要がある？もし明日もダメなら大学に聞きに行く
*** WAIT amazon返品
*** INBOX 家契約書
*** INBOX 家の電気ガス水道などの契約を調べておくこと→わからなければメールすること
*** INBOX kaggle
*** DONE 楽天カードポイント
*** WAIT 楽天カードを使用する→使ったけどよくわかんない
*** INBOX SLT周辺[0/9]
- [ ] [[https://arxiv.org/abs/2511.16893][[2511.16893] Predicting the Formation of Induction Heads]]
- [ ] [[https://arxiv.org/abs/2506.11135v1][[2506.11135v1] Large Language Models and Emergence: A Complex Systems Perspective]]
- [ ] [[https://arxiv.org/abs/2511.15419][[2511.15419] Singular Learning Theory for Factor Analysis]]
- [ ] [[https://openai.com/index/understanding-neural-networks-through-sparse-circuits/][Understanding neural networks through sparse circuits | OpenAI]]
- [ ] [[https://arxiv.org/abs/2307.02129][[2307.02129] How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model]]
- [ ] [[https://arxiv.org/abs/2510.24256][[2510.24256] From Memorization to Reasoning in the Spectrum of Loss Curvature]]
- [ ] [[https://joisino.hatenablog.com/entry/heads][LLM のアテンションと外挿 - ｼﾞｮｲｼﾞｮｲｼﾞｮｲ]]
- [ ] https://openreview.net/pdf?id=5JcDVsV8pf
- [ ] SGLD



* LOG
1250ごろに起床，1345に出発，
1409に volmetric出勤→18時過ぎに退勤して，その後ぶらぶらした
1930ごろにイオンのマクドナルドに来た．

chatGPTを使って，今の研究分野についてDeepResearchさせたものの，
正直自分が読んだ論文くらいしか見つけてきていない．プロンプトが悪いのかもしれない？
とにかく，自分が持っている課題に対しては今思いついている手法を試すのがstraight forwardであることがわかってきたので，とにかく手を動かすこと．

* タスク整理
** *研究*
とにかく何にも進捗がない．

from local to global 
https://arxiv.org/abs/2507.21449

*** TODO 論文まとめの記事を公開すること
*** TODO 研究の流れと解析について今考えていることを言語化する


直感的な考えも大切にする→定式化ができればOK
まず必要な予備実験を終える．その後に以下の質問に答えていく

そもそもこの研究の新規性はどこにある？
モジュールの分け方次第になってしまう
ファインチューニングを見た場合にどう使える？
事前学習モデル→所望タスクへのファインチューニング

** *arcanain*
今週ですべて終わる．

** *volmetric*
*** TODO windows emacs 環境構築しときたい
*** DONE 来週以降のシフト決定すること．
**** TODO case B implement
**** TODO CASE c Implement
**** TODO ここまで来たら，それぞれもう少し洗練させていく．モジュールの構成なども考え直す
**** TODO application to loop connecter
**** TODO そもそもシフト計算にはなんの最適化計算も入っていない
しかしながら，小さな部分の調整などはシフト計算が行ってもよいのか？
** *arcra*
契約書早くしろ
- https://qiita.com/ShotaDeguchi/items/d2e08c135f2eebaa624b
- https://qiita.com/probabilityhill/items/9a22f395a1e93206c846
- https://github.com/shiryu-nakano/ocr_project/blob/main/README.md
- https://chatgpt.com/c/69314ed4-7c9c-8322-ad93-46caa44d4234
**** 良心的な目標設定
**** 挑戦的な目標設定
**** 実験走行 何回目で確認走行区間を突破するのかの目標を決める
そこから逆算する


** *MISC*
*** TODO 終わったプロジェクトを移動したい
*** DONE パスポートの写真
*** DONE arcraの金の計算
*** TODO ARCRAの作業を可能にするための環境構築
**** DONE pyenv install
**** DONE python version install
**** TODO doc2の実装→今やっても意味ないので来週からやること
google vision ocr の使い方とかを適当に調べて実装すること
*** DONE 全体の計算への影響を見る
*** DONE volの12 月シフト提出．
*** DONE １２月シフトを給与計算に反映すること
*** TODO 加えてARCRAも計画すること．

*** DONE 桂井連絡
*** DONE 元武連絡
*** DONE 通学証明書の連絡を送ること
**** DONE 明日もらってから定期を作る
*** TODO 口座
口座の番号，つまりデビッドを登録するのか，もしくはクレジットカードを使うのかによってかなり変わってくるはず
*** TODO 引き落としの金額とかをチェックしておくこと
*** TODO 
** *routine*
** 読み物
*** INBOX [2310.06301] Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition
*** INBOX [2308.12108v2] The Local Learning Coefficient: A Singularity-Aware Complexity Measure
*** INBOX Daniel Murfet氏による特異学習理論(SLT)解説を読んだ感想」Interview Daniel Murfet on Universal Phenomena in Learning Machines — LessWron」
*** INBOX [論文レビュー] Abstract Gradient Training: A Unified Certification Framework for Data Poisoning, Unlearning, and Differential Privacy
- https://doshisha-iml-k.slack.com/archives/D08KF5H8KGF/p1765267578753069
- 
Singular Learning Theory — LessWrong

「Deep Learning is Singular, and That's Good」論文とpyro実装を読む
[2502.05475] You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation


Neural networks trained with SGD learn distributions of increasing complexity
SGD on Neural Networks Learns Functions of Increasing Complexity
Neural networks trained with SGD learn distributions of increasing complexity

Task-Specific Skill Localization in Fine-tuned Language Models

Explosive neural networks via higher-order interactions in curved statistical manifolds | Nature Communications

The cost of unmodeled biological complexity in artificial neural networks: Patterns

* 所感

