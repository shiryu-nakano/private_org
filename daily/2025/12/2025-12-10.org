#+title: 2025-12-10
#+filetags: :daily:

* TASK
** INBOX LT大会パワポ
** INBOX レポート作成→つくば
** INBOX ポスター作成
** INBOX レポート12月分の作成
*** 1._結果と反省
*** 2._今後の展望
** DONE 津田さんに結果報告メッセージ→今書いてスケジュールで送る
** misc
*** DONE 定期申し込み
*** DONE パスポート顔写真
*** WAIT 奨学金
→朝やる必要がある？もし明日もダメなら大学に聞きに行く


*** WAIT amazon返品
*** INBOX 家契約書
*** INBOX 家の電気ガス水道などの契約を調べておくこと→わからなければメールすること
*** INBOX kaggle
*** DONE 楽天カードポイント
*** WAIT 楽天カードを使用する→使ったけどよくわかんない
*** INBOX SLT周辺[0/9]
- [ ] [[https://arxiv.org/abs/2511.16893][[2511.16893] Predicting the Formation of Induction Heads]]
- [ ] [[https://arxiv.org/abs/2506.11135v1][[2506.11135v1] Large Language Models and Emergence: A Complex Systems Perspective]]
- [ ] [[https://arxiv.org/abs/2511.15419][[2511.15419] Singular Learning Theory for Factor Analysis]]
- [ ] [[https://openai.com/index/understanding-neural-networks-through-sparse-circuits/][Understanding neural networks through sparse circuits | OpenAI]]
- [ ] [[https://arxiv.org/abs/2307.02129][[2307.02129] How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model]]
- [ ] [[https://arxiv.org/abs/2510.24256][[2510.24256] From Memorization to Reasoning in the Spectrum of Loss Curvature]]
- [ ] [[https://joisino.hatenablog.com/entry/heads][LLM のアテンションと外挿 - ｼﾞｮｲｼﾞｮｲｼﾞｮｲ]]
- [ ] https://openreview.net/pdf?id=5JcDVsV8pf
- [ ] SGLD


*** DONE res to 本武
*** DONE res to 広田
*** DONE res to DOT.R

*** INBOX SMS何かの登録
https://mail.google.com/mail/u/0/?tab=rm&ogbl#search/%E3%81%A9%E3%81%A3%E3%81%A8/FMfcgzQdzcvBHrxWzFTsmSdnbhGghPTh

*** INBOX 必要書類
合格通知書写し
住民票（発行2ヶ月以内・マイナンバー記載なし・本籍地記載なし）
* LOG
** ながれ
- 0930起床
  1h?
  1043出発
  1056東福寺到着
  1101〜1144興戸に乗る
  1208研究室到着
  - 電車に乗る間→ベイズ統計学読んだ
  - 電車に乗る前→ラジオ聞いた（BBC pod cast）
  - 電車から大学→音楽きいた
- 大学到着後1210〜
  - 奨学金などの申請
    リレーロ口座おの加入手続き
  - 新居の書類にハンコをポンポンした．
  - 死ぬほど時間かかって，1430になった．
- ゼミ1455
  - メール返信ライン返信
  - ゼミ終了，1625
  - その後カフェに行った   
- 1726 作業開始
  - 論文読み
  - 研究の背景・思想・意義などはなーんとなくわかった．
  - つぎ，
    - 具体的に何を開発したのか？

    - それによってなにがかのうになったのか
      この論文んはデモンストレーション

    - 研究室から新田辺へのバス，1840
    - 1908
      帰りの電車でも続き
      - 三条に1945に到着(1h05m)

- 2255家に帰る

  - 三条→東福寺→ちゃりでいえまで
	
#+ATTR_ORG: :width 40
[[file:2025-12-10.org_image/20251210_203047.png]]


** 2025-12-10 17:40  :B4:
*** 論文よんだ
https://arxiv.org/pdf/2410.02984#page=4.75
**** 概要
個人的にはXAIの研究だと思っている．（論文では，"Interpretable", "develomental interpretability"という言葉が使われている）

この研究ではまず，Singular Learning Theory（SLT）に基づくLocal Learning Coefficient（LLC）を改良した．refined LLC（rLLC）というものを開発した．LLCはSLTに基づく（特異なものも含む）モデルの複雑性を定量的に図る指標のこと．


懸念
- 焼き直しになるかもしれない
- そもそもこの研究自体，多分　LLCの有用性を主張するためのデモンストレーションみたいな感じになっていて，
それに乗っかってしまっていいものか・・・

**** 解析手法の提案（rLLC）

***** LLCとrLLC
LLCについては他の文献を参照．
いままで省略してきたが，rLLCはweight refined LLC（wrLLC）とdata refined LLC（drLLC）の二つがある．
LLC（Lau et al.,2023 Local Learning Coeffitient:...）は
モデルのパラメータ全てに対して適用＋トレーニングデータ全てを用いて計算する．
これに対して，rLLCではパラメータ部分集合に適用する（例えば層，やattention head）＋データをトレーニングに用いていなものも用いる．後者がわかりずらいがアイデアとしてはキモかもしれない．

****** Weight-Refined LLC (wrLLC)
- Attention head が学習過程で どのように機能的に分化（differentiation）していくかを捉える指標である。
- wrLLC は「その head の重みだけを微小に変化させたときの損失の変化量」を測っており、
- 値が大きいほど損失地形が鋭く、パラメータの自由度が小さい＝より複雑で情報量の多い役割を担っていると解釈できる

****** Data-Refined LLC (drLLC)
- Attention head が学習過程で どのように特定のデータ分布に対して特殊化（specialization）していくかを捉える指標である。
- 特殊化とは、ある head が特定のデータ分布（例：自然言語、コード、数学文献など）が持つ固有の構造を扱う際に重要な役割を果たすようになることを指す。
- drLLC は、データ分布を変えたときの損失地形の変化から、その head がどの分布の構造に敏感か・依存しているかを明らかにする
  


**** 実験・結果

この研究では2層attention only transformerを用いた実験を行った．
実験では，モデルパラメータを部分集合に分割する．8つのattention headそれぞれを部分集合とし．モジュールと読んでいる．
そしてrLLCをモジュールごとに学習全体を通じて推定し，その時系列を得た．

既存の手法を用いてheadの役割をクラスタリングした結果と，rLLCのトレースによって視覚的に見られた違いが一致していることから，
rLLCのトレースは，モジュールの発達過程と，その結果得る役割の変化を強く反映していることがわかった．
なのでこの量を追跡することでモデルの発達を解析することができるということを主張している．

ここまでだと，rLLCによって新しくできることがないように思われるが，上記に加えて，multigram predictionを行うheadを発見した．これは時系列でrLLCを追跡することでしか発見できなかった（らしい）．

> The identification of a novel multigram circuit: refined LLCs reveal evidence of internal
structure related to multigram prediction, which we corroborate with other interpretability
techniques (Section 4.3).



**** 議論
細かなlimitationはたくさんある，，，今後追記に期待．
**** 感想
この手法を使って他の課題でモデルのdevelopmental interpretabilityに取り組みたい．
critical period周辺の研究にも使えるかもしれない．

データ分布q, q'の複数を用いるのは，うまくやれば応用側の研究で，実用的な知見が得られるかもしれないと思います．
ある言語間とかドメイン間ではデータの分布が異なるはずなので，それと事前学習済みモデルの内部表現を使って解析すれば，学習がうまくいく（いかない）理由の説明になって，その上で効果的な対処法を提案できたりする？
何かいいアイデアがあったらぜひ声をかけてほしいです．



***** NEXTSTEP
**** めも

  - 論文読み
  - 研究の背景・思想・意義などはなーんとなくわかった．
  - つぎ，
    - 具体的に何を開発したのか？

    - それによってなにがかのうになったのか
      この論文んはデモンストレーション


 LLCが低い＝簡単／高い＝難しい、という直感をもっと説明してほしい」

「rLLCを自分のモデルでどう計算すればいいか教えてほしい」

「この方法論があなたの研究テーマ（DSB・内部表現発達）にどう関係するか」
*
** 2025-12-10 22:11 論文読み終わった
次やるべきことは？
→先生に話に行く時の文章を書く→明日中に提出する必要がある．



* タスク整理
** *研究*
とにかく何にも進捗がない．

from local to global 
https://arxiv.org/abs/2507.21449

*** TODO 論文まとめの記事を公開すること
*** TODO 研究の流れと解析について今考えていることを言語化する


直感的な考えも大切にする→定式化ができればOK
まず必要な予備実験を終える．その後に以下の質問に答えていく

そもそもこの研究の新規性はどこにある？
モジュールの分け方次第になってしまう
ファインチューニングを見た場合にどう使える？
事前学習モデル→所望タスクへのファインチューニング

** *arcanain*
今週ですべて終わる．

** *volmetric*
*** TODO windows emacs 環境構築しときたい
*** DONE 来週以降のシフト決定すること．
**** TODO case B implement
**** TODO CASE c Implement
**** TODO ここまで来たら，それぞれもう少し洗練させていく．モジュールの構成なども考え直す
**** TODO application to loop connecter
**** TODO そもそもシフト計算にはなんの最適化計算も入っていない
しかしながら，小さな部分の調整などはシフト計算が行ってもよいのか？
** *arcra*
契約書早くしろ
- https://qiita.com/ShotaDeguchi/items/d2e08c135f2eebaa624b
- https://qiita.com/probabilityhill/items/9a22f395a1e93206c846
- https://github.com/shiryu-nakano/ocr_project/blob/main/README.md
- https://chatgpt.com/c/69314ed4-7c9c-8322-ad93-46caa44d4234
**** 良心的な目標設定
**** 挑戦的な目標設定
**** 実験走行 何回目で確認走行区間を突破するのかの目標を決める
そこから逆算する


** *MISC*
*** TODO 終わったプロジェクトを移動したい
*** DONE パスポートの写真
*** DONE arcraの金の計算
*** TODO ARCRAの作業を可能にするための環境構築
**** DONE pyenv install
**** DONE python version install
**** TODO doc2の実装→今やっても意味ないので来週からやること
google vision ocr の使い方とかを適当に調べて実装すること
*** DONE 全体の計算への影響を見る
*** DONE volの12 月シフト提出．
*** DONE １２月シフトを給与計算に反映すること
*** TODO 加えてARCRAも計画すること．

*** DONE 桂井連絡
*** DONE 元武連絡
*** DONE 通学証明書の連絡を送ること
**** DONE 明日もらってから定期を作る
*** TODO 口座
口座の番号，つまりデビッドを登録するのか，もしくはクレジットカードを使うのかによってかなり変わってくるはず
*** TODO 引き落としの金額とかをチェックしておくこと
*** TODO 
** *routine*
** 読み物
*** INBOX [2310.06301] Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition
*** INBOX [2308.12108v2] The Local Learning Coefficient: A Singularity-Aware Complexity Measure
*** INBOX Daniel Murfet氏による特異学習理論(SLT)解説を読んだ感想」Interview Daniel Murfet on Universal Phenomena in Learning Machines — LessWron」
*** INBOX [論文レビュー] Abstract Gradient Training: A Unified Certification Framework for Data Poisoning, Unlearning, and Differential Privacy
- https://doshisha-iml-k.slack.com/archives/D08KF5H8KGF/p1765267578753069
- 
Singular Learning Theory — LessWrong

「Deep Learning is Singular, and That's Good」論文とpyro実装を読む
[2502.05475] You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation


Neural networks trained with SGD learn distributions of increasing complexity
SGD on Neural Networks Learns Functions of Increasing Complexity
Neural networks trained with SGD learn distributions of increasing complexity

Task-Specific Skill Localization in Fine-tuned Language Models

Explosive neural networks via higher-order interactions in curved statistical manifolds | Nature Communications

The cost of unmodeled biological complexity in artificial neural networks: Patterns

* 所感
卒論の振り返りとして読んだ文献全てのメモを公開するか

#+ATTR_ORG: :width 400
[[file:2025-12-10.org_image/20251210_220958.png]]

4時間弱，論文読みに使っている．
