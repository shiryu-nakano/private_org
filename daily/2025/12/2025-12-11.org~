#+title: 2025-12-11
#+filetags: :daily:

* TASK
** INBOX LT大会パワポ
** INBOX レポート作成→つくば
** INBOX ポスター作成
** INBOX レポート12月分の作成
*** 1._結果と反省
*** 2._今後の展望
** misc
*** WAIT amazon返品
*** NEXT 家契約書
*** INBOX 家の電気ガス水道などの契約を調べておくこと→わからなければメールすること
*** INBOX kaggle
*** DONE 楽天カードポイント
*** WAIT 楽天カードを使用する→使ったけどよくわかんない
*** INBOX SLT周辺[0/9]
- [ ] [[https://arxiv.org/abs/2511.16893][[2511.16893] Predicting the Formation of Induction Heads]]
- [ ] [[https://arxiv.org/abs/2506.11135v1][[2506.11135v1] Large Language Models and Emergence: A Complex Systems Perspective]]
- [ ] [[https://arxiv.org/abs/2511.15419][[2511.15419] Singular Learning Theory for Factor Analysis]]
- [ ] [[https://openai.com/index/understanding-neural-networks-through-sparse-circuits/][Understanding neural networks through sparse circuits | OpenAI]]
- [ ] [[https://arxiv.org/abs/2307.02129][[2307.02129] How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model]]
- [ ] [[https://arxiv.org/abs/2510.24256][[2510.24256] From Memorization to Reasoning in the Spectrum of Loss Curvature]]
- [ ] [[https://joisino.hatenablog.com/entry/heads][LLM のアテンションと外挿 - ｼﾞｮｲｼﾞｮｲｼﾞｮｲ]]
- [ ] https://openreview.net/pdf?id=5JcDVsV8pf
- [ ] SGLD


*** DONE mail to DOT.R
*** INBOX ドイツ語の先生に連絡する
*** INBOX SMS何かの登録
https://mail.google.com/mail/u/0/?tab=rm&ogbl#search/%E3%81%A9%E3%81%A3%E3%81%A8/FMfcgzQdzcvBHrxWzFTsmSdnbhGghPTh

*** INBOX 必要書類
- 合格通知書写し
- 住民票（発行2ヶ月以内・マイナンバー記載なし・本籍地記載なし）



* LOG
** ながれ
- 0930起床
  1h?
  1045出発
  1103駅到着（東福寺）→

  - 電車にて，
    - 森氏に連絡する必要がある
    - jr遅れすぎ、1218とかにようやく同志社前
    - 1223→1245とかにlab到着

- lab到着
  1303〜
  雑務30分ほどで片付けたい
  雑務片付けた．次の休憩でドイツ語の先生に連絡すること．
- 3限のドイツ語が休校になったので研究を進められる．
- 草稿執筆
  
  - [ ] LLC の説明は不要？
  - [ ] 研究の概要
    - どんなトピックで，どんなことをしたいのか
  - [ ] 研究の背景
    - [ ] DSBの研究
    - [ ] increasing complexity
      - ここではDSBとmomentをどのように位置付けているのか？
	- 
	
    - [ ] moment

  - LLC の話
    - [ ] wrLLCの説明
    - [ ] drLLCの説明
    - [ ] 実際この研究で何がわかったのか
  - 研究でやりたいことを再度書く
  - 手法をまとめる
  - 結果→まだ

    
 


* タスク整理
** *研究*

from local to global 
https://arxiv.org/abs/2507.21449
*** TODO 研究の流れと解析について今考えていることを言語化する


直感的な考えも大切にする→定式化ができればOK
まず必要な予備実験を終える．その後に以下の質問に答えていく

そもそもこの研究の新規性はどこにある？
モジュールの分け方次第になってしまう
ファインチューニングを見た場合にどう使える？
事前学習モデル→所望タスクへのファインチューニング




最優先今日中にドキュメント作成



** *arcanain*
今週ですべて終わる．

** *volmetric*
*** TODO windows emacs 環境構築しときたい
*** DONE 来週以降のシフト決定すること．
**** TODO case B implement
**** TODO CASE c Implement
**** TODO ここまで来たら，それぞれもう少し洗練させていく．モジュールの構成なども考え直す
**** TODO application to loop connecter
**** TODO そもそもシフト計算にはなんの最適化計算も入っていない
しかしながら，小さな部分の調整などはシフト計算が行ってもよいのか？
** *arcra*
契約書早くしろ
- https://qiita.com/ShotaDeguchi/items/d2e08c135f2eebaa624b
- https://qiita.com/probabilityhill/items/9a22f395a1e93206c846
- https://github.com/shiryu-nakano/ocr_project/blob/main/README.md
- https://chatgpt.com/c/69314ed4-7c9c-8322-ad93-46caa44d4234
**** 良心的な目標設定
**** 挑戦的な目標設定
**** 実験走行 何回目で確認走行区間を突破するのかの目標を決める
そこから逆算する


** *MISC*
*** TODO 終わったプロジェクトを移動したい
*** DONE パスポートの写真
*** DONE arcraの金の計算
*** TODO ARCRAの作業を可能にするための環境構築
**** DONE pyenv install
**** DONE python version install
**** TODO doc2の実装→今やっても意味ないので来週からやること
google vision ocr の使い方とかを適当に調べて実装すること
*** DONE 全体の計算への影響を見る
*** DONE volの12 月シフト提出．
*** DONE １２月シフトを給与計算に反映すること
*** TODO 加えてARCRAも計画すること．

*** DONE 桂井連絡
*** DONE 元武連絡
*** DONE 通学証明書の連絡を送ること
**** DONE 明日もらってから定期を作る
*** TODO 口座
口座の番号，つまりデビッドを登録するのか，もしくはクレジットカードを使うのかによってかなり変わってくるはず
*** TODO 引き落としの金額とかをチェックしておくこと
*** TODO 
** *routine*
** 読み物
*** INBOX [2310.06301] Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition
*** INBOX [2308.12108v2] The Local Learning Coefficient: A Singularity-Aware Complexity Measure
*** INBOX Daniel Murfet氏による特異学習理論(SLT)解説を読んだ感想」Interview Daniel Murfet on Universal Phenomena in Learning Machines — LessWron」
*** INBOX [論文レビュー] Abstract Gradient Training: A Unified Certification Framework for Data Poisoning, Unlearning, and Differential Privacy
- https://doshisha-iml-k.slack.com/archives/D08KF5H8KGF/p1765267578753069
- 
Singular Learning Theory — LessWrong

「Deep Learning is Singular, and That's Good」論文とpyro実装を読む
[2502.05475] You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation


Neural networks trained with SGD learn distributions of increasing complexity
SGD on Neural Networks Learns Functions of Increasing Complexity
Neural networks trained with SGD learn distributions of increasing complexity

Task-Specific Skill Localization in Fine-tuned Language Models

Explosive neural networks via higher-order interactions in curved statistical manifolds | Nature Communications

The cost of unmodeled biological complexity in artificial neural networks: Patterns

* 所感
卒論の振り返りとして読んだ文献全てのメモを公開するか

#+ATTR_ORG: :width 400
[[file:2025-12-10.org_image/20251210_220958.png]]

やっぱしiPhoneが良くないみたいだ
- orgでもチェックボックスは使える
  https://oversleptabit.com/archives/5842
C-cでチェック切り替えができる
