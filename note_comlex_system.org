* 15:30–15:50   佐原恭平 (株式会社 COTEN, 京都大学大学院エネルギー科学研究科)



3Reasning
perplexityを下げると思考しなくなって即答する→？

RAG

学習していない内容をやらせたい

Reasingすべきかいなか


コンテキストがはっきりしている方がいいのか，そうではないのか
perplexity
reasningしているかどうかを測る指標はある？

* 15:50–16:20   飯塚博幸（北大）
北大

他者から自己へと立ち上がる心
- CHAIN

Mithcel waldrop 複雑系
複雑系の進化的シナリオ

方法論・わかり方への探求
→新しい理解の仕方
→→LLMなどAIと，人間での理解の仕方の違いを見てみるとどうなるか

オートピエーシス
絶対矛盾的自己同一との関連がありそう

えなくテぃヴィズむ


構成論的アプローチ
対象が成立する最小構成を実施に作ることで理解する→構成論的アプローチで，
複雑系は研究されてきた

open end絶え間ないノイズ，非同期的インタラクション

LLMもその一部として考えられないのか？

→構成論的あpに対して，LLMなどはリアルワールドで直接比較できるようになった

錯視はNNでも再現できる
→目隠しして動かすと視覚情報がなくても刺激を予測する


theory theory, Simulation Theory

Interaction Theory


encoder1，2は学習した後に明らかに異なる関数になるのか？？


モデルをこんなに自由に作っていいと言うのが感動した点

自他が未分→というモデルを作ることで自分の中に他者の情報もふくまれる．

ミラーニューロン

** 所感
モデルの自由が面白い

transformerとの関連性

飯塚さん→モジュール増やした場合にエージェントごとの情報は抜き出せなくても，
外部の2つのエージェントが作る図形みたいな情報を抜き出すことはできない？

挟まれているのかor 前に二人いる

自転車にのってるときに，一人ずつ意識するっていうよりかは，
むしろ他者を総体化して図形？としてみることで自分の進む道を選んでいる気がする

すごく気になるのは，DWAとの関係！！懇親会の時に話しかけてみたい


* 16:20–16:50   國吉康夫（東大）

母親

** 所感

 SLT→？？

 学習方法と関係を見るが．．．

 損失関数の話
教育の賜物だとしたら損失関数の設計とネットワークの初期値というか，なんらかのバイアスの話にしかならない
ネットワークの初期値の分布とSimpicity Bias





* 16:50–17:50   Discussion
